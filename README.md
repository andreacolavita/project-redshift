# Project 3: Cloud Data Warehouse (AWS Redshift)
In this project we are helping a music streaming startup to analyze what songs the users are listening to. Their data resides in Amazon S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

We are creating an ETL pipeline that extracts their data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for their analytics team to continue finding insights in what songs their users are listening to.

## Prerequisites (AWS)

To access AWS from the python code need first:
1. create IAM user 
2. create IAM role with AmazonS3ReadOnlyAccess access rights
3. get ARN
4. create and run Redshift cluster 

In the **dwh.cf** config file we have all the configuration parametres required to access and configure AWS.   

## Data Source

### Song Dataset
The first dataset is a subset of real data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.

Below is an example of what a single song file looks like.

`
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
`

### Log Dataset
The second dataset consists of log files in JSON format generated by an event simulator based on the songs in the dataset above.

The log files in the dataset is partitioned by year and month. For example, here are filepaths to two files in this dataset.

`
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
`

And below is an example of what the data in a log file, 2018-11-12-events.json, looks like.

[log_data](images/log-data.png)

## Schema Design

Analytics database (sparkifydb) schema has a star design: it has one Fact Table having business data, and supporting Dimension Tables. The Fact Table answers one of the key questions: what songs users are listening to.

### Fact Table
**songplays**: song play data together with user, artist, and song info (songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent)

The songplays table is distribuited and sorted by user_id.

### Dimension Tables
**users**: user info (columns: user_id, first_name, last_name, gender, level)

Users table is a quite small dimension table and for this reason is distribuited with style ALL: a copy of this table is replicated in all the nodes to speed-up the join with the fact table. 

**songs**: song info (columns: song_id, title, artist_id, year, duration)

Songs table doesn't have any distribuition style and it means that will have AUTO by default. Redshift will understand if it is a big table to assign it EVEN (the leader node distributes the rows across the slices in a round-robin fashion) or a small table to assign it ALL (a copy of the entire table is distributed to every node. ). 

**artists**: artist info (columns: artist_id, name, location, latitude, longitude)

Artists table is a quite small dimension table and for this reason is distribuited with style ALL.

**time**: detailed time info about song plays (columns: start_time, hour, day, week, month, year, weekday)

Time table is a quite small dimension table and for this reason is distribuited with style ALL.

## ETL pipeline

Project has two scripts:

+ **create_tables.py**: This script drops existing tables and creates new ones.
+ **etl.py**: This script uses data in s3:/udacity-dend/song_data and s3:/udacity-dend/log_data, processes it, and inserts the processed data into DB.


